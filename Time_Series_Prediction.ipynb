{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "from itertools import product\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.graphics.tsaplots import plot_pacf,plot_acf\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARIMA\n",
    "Lags -> In auto regressive there are some value which is calleed lags, suppose today our output is Y it is actually dependent on some data on previous day so those previous days are lags.\n",
    "\n",
    "Seasonal data -> In time series data, seasonality is the presence of variations that occur at specific regular intervals less than a year, such as weekly, monthly, or quarterly. Seasonality may be caused by various factors, such as weather, vacation, and holidays and consists of periodic, repetitive, and generally regular and predictable patterns in the levels of a time series.\n",
    "\n",
    "In order to check our data is stationary or not by seeing the mean and the variance weather it almost same we basically apply something called Dickey–Fuller test.\n",
    "\n",
    "Dickey–Fuller -> In statistics, the Dickey–Fuller test tests the null hypothesis that a unit root is present in an autoregressive time series model. The alternative hypothesis is different depending on which version of the test is used, but is usually stationarity or trend-stationarity. The test is named after the statisticians David Dickey and Wayne Fuller, who developed it in 1979.\n",
    "\n",
    "NullHypothesis -> In inferential statistics, the null hypothesis is that two possibilities are the same. The null hypothesis is that the observed difference is due to chance alone. Using statistical tests, it is possible to calculate the likelihood that the null hypothesis is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('Infosys_Stock_Price_Dataset.xlsx', usecols=['Date', 'High'])\n",
    "# df['Date'] = pd.to_datetime(df['Date'])\n",
    "# df.set_index('Date',inplace=True)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(x = 'Date', y = 'High', figsize=(16,8), title='Infosys daily stock price', grid=True, ylabel='High price (INR)', colormap='plasma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use adfuller from stats model for the dickey-fuller test. It takes the entire dataset and return five different values those are 'ADF Test Statistic','p-value','#Lags Used','Number of Observations Used'. The most important thing we require is p-value. Dickey-fuller test it is kind of hypothesis testing where in the Null hypothesis says that the data is not a stationary where as my alternate hypothesis says that it is stationary so, based on this particular condition is my p-value is < 0.05 it basically says that Data has no unit root and is stationary that means if we are getting 0.05 we are rejecting Nullhypothesis and if we are rejecting the nullhypothesis by deafult the alternate hypothesis get selected that is called as it is stationary. \n",
    "If our data is non stationary then we have to make our data stationary for that we can do Differencing.\n",
    "\n",
    "Differencing -> In this method we are shifting the required number position of the price data or required data filed that means that number of record will move down. Once our data is stationary that means we are rejecting Null hypothesis and accepting the alternate hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ho: It is non stationary\n",
    "#H1: It is stationary\n",
    "\n",
    "def adfuller_test(price):\n",
    "    result=adfuller(price)\n",
    "    labels = ['ADF Test Statistic','p-value','#Lags Used','Number of Observations Used']\n",
    "    for value,label in zip(result,labels):\n",
    "        print(f'{label}: {value:.4f}')\n",
    "    if result[1] <= 0.05:\n",
    "        print(\"strong evidence against the null hypothesis(Ho), reject the null hypothesis. Data has no unit root and is stationary\")\n",
    "    else:\n",
    "        print(\"weak evidence against null hypothesis, time series has a unit root, indicating it is non-stationary \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adfuller_test(df['High'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autocorrelation_plot(df['High'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['stationary_high'] = df['High'] - df['High'].shift(1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adfuller_test(df['stationary_high'].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['stationary_high'].plot(figsize=(16,8), title='Infosys daily stock price', grid=True, ylabel='High price (INR)', colormap='plasma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autocorrelation_plot(df['stationary_high'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autocorrelation and Partial Autocorrelation\n",
    "Identification of an AR model is often best done with the PACF.\n",
    "\n",
    "For an AR model, the theoretical PACF “shuts off” past the order of the model. The phrase “shuts off” means that in theory the partial autocorrelations are equal to 0 beyond that point. Put another way, the number of non-zero partial autocorrelations gives the order of the AR model. By the “order of the model” we mean the most extreme lag of x that is used as a predictor.\n",
    "Identification of an MA model is often best done with the ACF rather than the PACF.\n",
    "\n",
    "For an MA model, the theoretical PACF does not shut off, but instead tapers toward 0 in some manner. A clearer pattern for an MA model is in the ACF. The ACF will have non-zero autocorrelations only at lags involved in the model.\n",
    "p -> AR model lags\n",
    "d -> differencing\n",
    "q -> MA lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "ax1 = fig.add_subplot(211)\n",
    "fig = sm.graphics.tsa.plot_acf(df['stationary_high'].iloc[1:],lags=40,ax=ax1)\n",
    "ax2 = fig.add_subplot(212)\n",
    "fig = sm.graphics.tsa.plot_pacf(df['stationary_high'].iloc[1:],lags=40,ax=ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_pdq(df_data):\n",
    "    flt_best_aic = None\n",
    "    tup_best_pdq = None\n",
    "    p = d = q = range(0, 5)\n",
    "    lst_pdq = list(product(p, d, q))\n",
    "    for tup_pdq in tqdm_notebook(lst_pdq):\n",
    "        try:\n",
    "            model_arima = ARIMA(df_data, order = tup_pdq)\n",
    "            model_arima_fit = model_arima.fit()\n",
    "            if flt_best_aic is None:\n",
    "                flt_best_aic = model_arima_fit.aic\n",
    "            elif model_arima_fit.aic < flt_best_aic:\n",
    "                flt_best_aic = model_arima_fit.aic\n",
    "                tup_best_pdq = tup_pdq\n",
    "        except:\n",
    "            continue\n",
    "    return tup_best_pdq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_best_pdq(df['stationary_high'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ARIMA(df['High'], order=(4, 0, 4), trend='t')\n",
    "model_fit=model.fit()\n",
    "model_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_date = pd.date_range(start = df.Date.iloc[-1], periods=10, freq='D')\n",
    "future_date_df = pd.DataFrame(future_date[1:], columns = ['Date'])\n",
    "df = pd.concat([df, future_date_df], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = df.loc[2263:2282, :]\n",
    "test_df['Date'] = pd.to_datetime(test_df['Date'])\n",
    "test_df.set_index('Date', inplace = True)\n",
    "prediction = model_fit.predict(start = 2263, end = 2282)\n",
    "prediction.index = test_df.index\n",
    "# prediction = model_fit.predict(start = df.index[-15], end = df.index[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15, 7.5), dpi = 400)\n",
    "plt.xlabel('Date')\n",
    "plt.title('Infosys daily stock price')\n",
    "plt.ylabel('High price (INR)')\n",
    "test_df['High'].plot(color = 'Blue', label = 'Actual data')\n",
    "prediction.plot(color = 'red', label = 'ARIMA predicted')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(df.High.iloc[2263:2274], model_fit.predict(start = 2263, end = 2273))\n",
    "rmse = sqrt(mse)\n",
    "print(f'Mean squared error: {mse}')\n",
    "print(f'Root mean squared error: {rmse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARIMAX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = pd.read_excel('Infosys_Stock_Price_Dataset.xlsx', usecols=['Date', 'High'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.plot(x = 'Date', y = 'High', figsize=(16,8), title='Infosys daily stock price', grid=True, ylabel='High price (INR)', colormap='plasma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pacf(dfs['High'])\n",
    "plot_acf(dfs['High'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_fuller_result = adfuller(dfs['High'])\n",
    "print(f'ADF Statistic: {ad_fuller_result[0]}')\n",
    "print(f'p-value: {ad_fuller_result[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the p-value is large, we cannot reject the null hypothesis and must assume that the time series is non-stationary.\n",
    "Now, let’s take the log difference in an effort to make it stationary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['High'] = np.log(dfs['High'])\n",
    "dfs['High'] = dfs['High'].diff()\n",
    "dfs = dfs.drop(dfs.index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[15, 7.5]); # Set dimensions for figure\n",
    "plt.plot(dfs['High'])\n",
    "plt.title(\"Log Difference of Daily Infosys stock price\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the p-value is small enough for us to reject the null hypothesis, and we can consider that the time series is stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_fuller_result = adfuller(dfs['High'])\n",
    "print(f'ADF Statistic: {ad_fuller_result[0]}')\n",
    "print(f'p-value: {ad_fuller_result[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pacf(dfs['High'])\n",
    "plot_acf(dfs['High'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_SARIMA(parameters_list, d, D, s, exog):\n",
    "    \"\"\"\n",
    "        Return dataframe with parameters, corresponding AIC and SSE\n",
    "        \n",
    "        parameters_list - list with (p, q, P, Q) tuples\n",
    "        d - integration order\n",
    "        D - seasonal integration order\n",
    "        s - length of season\n",
    "        exog - the exogenous variable\n",
    "    \"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for param in tqdm_notebook(parameters_list):\n",
    "        try: \n",
    "            model = SARIMAX(exog, order=(param[0], d, param[1]), seasonal_order=(param[2], D, param[3], s)).fit(disp=-1)\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        aic = model.aic\n",
    "        results.append([param, aic])\n",
    "        \n",
    "    result_df = pd.DataFrame(results)\n",
    "    result_df.columns = ['(p,q)x(P,Q)', 'AIC']\n",
    "    #Sort in ascending order, lower AIC is better\n",
    "    result_df = result_df.sort_values(by='AIC', ascending=True).reset_index(drop=True)\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = range(0, 4, 1)\n",
    "d = 1\n",
    "q = range(0, 4, 1)\n",
    "P = range(0, 4, 1)\n",
    "D = 1\n",
    "Q = range(0, 4, 1)\n",
    "s = 4\n",
    "parameters = product(p, q, P, Q)\n",
    "parameters_list = list(parameters)\n",
    "print(len(parameters_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = optimize_SARIMA(parameters_list, 1, 1, 4, dfs['High'])\n",
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In statistics, AIC is used to compare different possible models and determine which one is the best fit for the data. AIC is calculated from: the number of independent variables used to build the model. the maximum likelihood estimate of the model (how well the model reproduces the data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minm = result_df.AIC.min()\n",
    "minm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = [indx for indx, itm enumerate(result_df.AIC) if itm == result_df.AIC.min()]\n",
    "result_df.iloc[idx[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = SARIMAX(dfs['High'], order=(3, 1, 3), seasonal_order=(1, 1, 3, 4)).fit(dis=-1)\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.plot_diagnostics(figsize=(15,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_date = pd.date_range(start = dfs.Date.iloc[-1], periods=10, freq='D')\n",
    "future_date_dfs = pd.DataFrame(future_date[1:], columns = ['Date'])\n",
    "dfs = pd.concat([dfs, future_date_dfs], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dfs = dfs.loc[2263:2282, :]\n",
    "test_dfs['Date'] = pd.to_datetime(test_dfs['Date'])\n",
    "test_dfs.set_index('Date', inplace = True)\n",
    "prediction_s = best_model.predict(start = 2263, end = 2282)\n",
    "prediction_s.index = test_dfs.index\n",
    "# prediction = model_fit.predict(start = df.index[-15], end = df.index[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15, 7.5), dpi = 400)\n",
    "plt.xlabel('Date')\n",
    "plt.title('Infosys daily stock price')\n",
    "plt.ylabel('High price (INR)')\n",
    "test_dfs['High'].plot(color = 'Blue', label = 'Actual data')\n",
    "prediction_s.plot(color = 'red', label = 'ARIMA predicted')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_s = mean_squared_error(dfs.High.iloc[2263:2274], best_model.predict(start = 2263, end = 2273))\n",
    "rmse_s = sqrt(mse_s)\n",
    "print(f'Mean squared error: {mse_s}')\n",
    "print(f'Root mean squared error: {rmse_s}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "968b7aa3b1eec891bdad5f4db6ea20f383fefc4491119e514896dff3fdf1f194"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
